{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Spark\n",
    "What is Spark? \n",
    " \n",
    " Spark is a distributed computing framework\n",
    "* Apache spark is a fast and general purpose engine for large scale data processing and it works on a cluster. It doesn't come with inbuilt cluster resource manager\n",
    "    * Spark Core(Heart of Spark)\n",
    "        * Cluster computing engine\n",
    "            * Memory Management,Task Scheduling, Faulty recovery and interact with cluster manager\n",
    "        * Set of core api's(libraries) supports java,python,scala,R\n",
    "            * Structured API's (Data Frames,Data Sets)\n",
    "            * Un-Structured API's(RDD)\n",
    "* <b> We feel that working on a database in good case, for the worst case we need work on collections </b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does spark executes our programs on a cluster\n",
    "* Master slave architecure\n",
    "    * Master is the Driver\n",
    "        * Driver is responsible for analysing, distributing, scheduling and monitoring work across the executors\n",
    "    * Slaves are executors\n",
    "* Once you submit your spark submit, the request goes to yarn resource manager and it launches application master.\n",
    "* The application master will launch a driver\n",
    "* The executor will perform task(one task for one partition and default spark creates one partition for one block)\n",
    "* When your program has transformations and actions then all transaformations will be in one stage and actions will be another stage because when you run your word count program , the results will be clubbed at reduce/reduceByKey that means all the partitions data will be shuffled and sorted then apply actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default spark submit creates one executor per one data node\n",
    "    * Suppose if the file size is around 600MB then file splits to 600MB/128MB(default block size) = 5 blocks\n",
    "        * default taks created =  5 tasks(one per block)\n",
    "        * default partitions = 5 partitions(one per block)\n",
    "        * default executors = 1(single node cluster)\n",
    "* <b> 1 core = 1 Thread/process </b>\n",
    "\n",
    "* <b> So we might think, more concurrent tasks for each executor will give better performance. But research shows that any application with more than 5 concurrent tasks, would lead to a bad show. So the optimal value is 5. </b>    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1 Hardware — \n",
    "* 6 Nodes and each node have 16 cores, 64 GB RAM\n",
    "    * 1 core and 1 gb of ram allocated to os and hadoop daemons\n",
    "    * so, 15 cores, 63 gb on each node available\n",
    "        * 15 cores * 6 nodes = 90 cores\n",
    "        * 90 cores/ 5 cores max per executor for max throughput = 18 executors\n",
    "        * 1 executor for Application Master(AM)\n",
    "        * 17 executors available / 6 node cluster = 3 executors for each data node\n",
    "        * Each node has 63 gb RAM available / 3 executors per datanode = 21 GB of RAM for each executor\n",
    "        * Spark Memory divided into 3 parts\n",
    "            * Storage Memory (75%)\n",
    "            * User Memory (25%)\n",
    "            * Reserved Memory (300Mb)\n",
    "        * 21GB*(1-0.07)=19GB available for each executor memory\n",
    "    * <b> So, Finally, --num-executors=17 --executor-memory=19gb --executor-cores=5 </b>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2 Hardware — \n",
    "* 6 Nodes and Each node have 32 Cores, 64 GB\n",
    " * 1 core and 1 gb of ram allocated to os and hadoop daemons\n",
    "    * so, 31 cores, 63 gb on each node available\n",
    "        * 31 cores * 6 nodes = 186 cores\n",
    "        * 186 cores/ 5 cores max per executor for max throughput = 37 executors\n",
    "        * 1 executor for Application Master(AM)\n",
    "        * 36 executors available / 6 node cluster = 6 executors for each data node\n",
    "        * Each node has 63 gb RAM available / 6 executors per datanode = 10 GB of RAM for each executor\n",
    "        * Spark Memory divided into 3 parts\n",
    "            * Storage Memory (75%)\n",
    "            * User Memory (25%)\n",
    "            * Reserved Memory (300Mb)\n",
    "        * 10GB*(1-0.07)=9GB available for each executor memory\n",
    "    * <b> So, Finally, --num-executors=37 --executor-memory=9gb --executor-cores=5 </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 3 Hardware -- When available memory more than required\n",
    "* Suppose in case 1 you think that 19 gb is more memory and you fell 10 gb is sufficient then calculation changes\n",
    "    * 10 gb per executor, so 63 gb available on each node / 10 = 6 executors are needed\n",
    "    * 6 executors * 6 nodes = total 36 executors\n",
    "    * 15 cores * 6 = 90 cores\n",
    "    * 90 cores / 36 executors = 3 cores per each executor\n",
    "    * Since for bettter optimization we can't increase no of cores for each executor\n",
    "    * <b> So, Finally, --num-executors=36 --executor-memory=10gb --executor-cores=3 </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Spark default read and write format to hive is parquet </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b> Spark default nature is cache that means spark decides to cache the frequently used data and this is in smal size </b>\n",
    "* <b> Persistence is that we ask spark to store the data either in ram/disk, this way spark stores large amount of data </b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where do we use vargs(*),kwargs(**)?\n",
    "* <b> *args are used in to convert all of data frame columns </b>\n",
    "\n",
    "     <code> \n",
    "     def reNameColumns(cols):\n",
    "                reNamedCols = []\n",
    "                for col in cols:\n",
    "                    reNamedCols.append(re.sub(\"[^a-zA-Z0-9]\", \"_\", col.lower()))\n",
    "                return reNamedCols\n",
    "     cols = reNameColumns(jsonDF.columns)\n",
    "     jsonDF.toDF(*cols)\n",
    "     </code>\n",
    "     \n",
    "* <b>**args are used in to get jdbc connection proprerties as map </b>\n",
    "     <code>\n",
    "         Class JDBCConnection:\n",
    "                 def __init__(self):\n",
    "                    self.config = cp.ConfigParser()\n",
    "                    self.config.read(\n",
    "                        r\"C:\\Users\\91889\\PycharmProjects\\pyspark_poc\\src\\main\\python\\com\\rposam\\spark\\config\\properties.txt\")\n",
    "\n",
    "            def getPostgresConnectoin(self):\n",
    "                host = self.config.get(\"postegres_props\", \"host\")\n",
    "                port = self.config.get(\"postegres_props\", \"port\")\n",
    "                user = self.config.get(\"postegres_props\", \"user\")\n",
    "                pwd = self.config.get(\"postegres_props\", \"password\")\n",
    "                driver = self.config.get(\"postegres_props\", \"driver\")\n",
    "                dbname = self.config.get(\"postegres_props\", \"dbname\")\n",
    "                props = {\n",
    "                    \"url\": \"jdbc:postgresql://{}:{}/{}\".format(host, port, dbname),\n",
    "                    \"user\": user,\n",
    "                    \"password\": pwd,\n",
    "                    \"driver\": driver,\n",
    "                    \"batchsize\": 25000\n",
    "                }\n",
    "                return props\n",
    "            props = JDBCConnection().getPostgresConnectoin()\n",
    "            jsonDF.coalesce(1).write.format(\"jdbc\"). \\\n",
    "                mode(saveMode=\"append\").options(**props).option(\"dbtable\", \"awspostgredb.json_tab\").save()\n",
    "     </code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's the most complicated issue you faced?\n",
    "* The most complicated issue i face is missing of encode for one of my task. I got a task to load some of delimiter separated files and join them and store the results into a jdbc.\n",
    "* The delimiter for the all the files are two characters but spark driver api read support only one delimiter\n",
    "* So , i did choose to load data as rdd instead of dataframe limitations(as above two delimeters)\n",
    "    <code>\n",
    "        --Failed \n",
    "          df = spark.read.format(\"csv\").option(\"delimiter\",\"::\").load(\"two_delimitors_sep_file.txt\")\n",
    "        --Success \n",
    "         rdd = spark.sparkContext.textFile(\"two_delimitors_sep_file.txt\")\n",
    "         rdd = rdd.map(lambda line: line.encode(\"utf-8\").map(lambda line:line.split(\"::\")).map(lambda line:(line(0),line(1),line(2),line(3)).toDF()\n",
    "    </code>       \n",
    "* Converted rdd to a dataframe and verifed all the files data showing as expected but whiel joining two dataframes i got an error(some espace character or other error) and got stuck there.\n",
    "* I try looking at datasets and printing some lines and everything is fine but join fails.\n",
    "* After struggling for some hours, somewhere i found that's an issue with rdd data read.\n",
    "* <b> After rdd created, first thing  we need to do is encoding (convert from unicode to string). </b>\n",
    "    <code>\n",
    "        rdd.map(lambda line:line.encode(\"utf-8\")\n",
    "    </code>\n",
    "* Then followed joining of all dataframes and finally succeeded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is lineage?\n",
    "* Lineage is a flow of transformations and spark remembers dependent of transformatons.\n",
    "* Lets consider word count program as an example.\n",
    "    <code>\n",
    "        rdd = spark.sparkContext.textFile(\".../dir\")\n",
    "        rdd1 = rdd.map(lamdba x : x.encode(\"utf-8\")\n",
    "        rdd2 = rdd1.flatMap(lambda x : x.split(\",\"))\n",
    "        rdd3 = rdd2.map(lambda x: (x,1))\n",
    "        rdd4 = rdd3.reduceByKey(lambda x,y:x+y)\n",
    "        rdd4.repartition(1).saveAsTextFile(\".../dir\")\n",
    "     </code>\n",
    "* As you know transformations are lazy and it is invoked only when an action being called. So, All transformations(rdd,rdd1,rdd2,rdd3) are one dependent to another and spark only remembers it , doesn't execute.\n",
    "* Series of dependent transformations are called lineage.\n",
    "* When rdd3.reduceByKey(this is an action) is called, spark tries to execute it's dependent transformations.\n",
    "            rdd1 = rdd.map --> rdd1 object has data and releases dependent objects(i.e rdd) data(free the memory)\n",
    "            rdd2 = rdd1.flatMap --> rdd2 object has data and free the data of rdd1\n",
    "            rdd3 = rdd2.map --> rdd3 has data and free the data of rdd2\n",
    "            rdd4 = rdd3.reduceByKey --> rdd3 has data and free the data of rdd3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is persistence and when do we go for persistence?\n",
    " Persistence is nothing but storing of data(either in memory or disk or both). \n",
    " \n",
    " In the above example, there is only one transformation exist, what if any of other actions exists?. \n",
    " let's see an example\n",
    "    <code>\n",
    "     r1 = sc.textFile(\".../dir\")\n",
    "     r2 = r1.t1()\n",
    "     r3 = r2.t2()\n",
    "     r4 = r3.t3()\n",
    "     r5 = r4.ac1()\n",
    "     r6 = r3.t4()\n",
    "     r7 = r6.ac2()    \n",
    "    </code>\n",
    "    \n",
    "   The above example, there are 2 actions and both are dependent on r3 but when first action(r4.ac1) executed it frees up the data in r3 and r6 starts execution again from beggining(i.e from textFile).\n",
    "   \n",
    "   Here r1 to r3 executes twice, so in-order to speed-up the executions we go for persisting(reusing) the r3 data and reduces execution time.\n",
    "   \n",
    "   <code>\n",
    "     r1 = sc.textFile(\".../dir\")\n",
    "     r2 = r1.t1()\n",
    "     r3 = r2.t2()\n",
    "     r3.persist(StorageLevel.Memory_only)\n",
    "     r4 = r3.t3()\n",
    "     r5 = r4.ac1()\n",
    "     r6 = r3.t4()\n",
    "     r7 = r6.ac2()    \n",
    "    </code>\n",
    "  <b> Persist will be executed only when whole file scan action executed, that means count,groupBy,Join,Set Operations</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of persistence?\n",
    "Pyspark has 6 types of persistence but java/scala has 8 types of persistence because java/scala supports storing the data deserialization mode where as python stores all the data in serialization mode.\n",
    "\n",
    "<b> What's Serialization an Deserialization? </b>\n",
    "####  Serialization is converting JVM object to binary object(This works in the same way of how compression works).\n",
    "\n",
    "* When we compress any of the file, compressed zip file will reduce the size of actual file to it's maximum.\n",
    "\n",
    "* When we download any of the file from internet, we prefer to download tar/zip compressed files, we don't download it's original unizipped/untar files.\n",
    "\n",
    "* In the same way when spark shuffles(redistribution) the data, to reduce network traffic and transfers serialized objects and deserialize it while processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What's DAG?\n",
    "DAG represents the flow of execution(transformations/actions).\n",
    "DAG shows execution flow with stages and each new stage is created when there's shuffle operations performed. \n",
    "\n",
    "Consider Word count program, the second stage is created when reduceByKey action performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's checkpoint?\n",
    "Entire information spark application execution including persisted data will be maintained with lineage. If memory gets crashed, lineage info also will be lost and spark forgets everything till it processed before crash.\n",
    "\n",
    "<b>Irrespective of lineage crash, is it possible to load intermittent data in somewhere at disk?</b>\n",
    "\n",
    "yes that is checkpointing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log4j.properties\n",
    "* We create a class of Log4j or use any available Log4j class\n",
    "* we need to include following in spark_home/conf/spark-defaults.conf\n",
    "    <code>\n",
    "    spark.driver.extraJavaOptions\t   -Dlog4j.configuration=file:log4j.properties -Dlogfile.name=filenameyourwish -Dspark.yarn.app.container.log.dir=app-logs\n",
    "    </code>\n",
    "* Log4j class accept sparksession and creates logger object\n",
    "    <code>\n",
    "    from com.rposam.pyspark.log.Log4j import Log4j\n",
    "\n",
    "    logger = Log4j(spark)\n",
    "    logger.info(\"Starting spark session...\")\n",
    "    </code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What spark do?\n",
    "* Spark is used to create programs and execute programs\n",
    "* How to execute spark programs?\n",
    "    * Interactive clients\n",
    "        * Spark shell\n",
    "        * notebook\n",
    "    * spark submit\n",
    "* <b>Spark is a Distributed processing engine, so how does it works?</b>\n",
    "    * When you submit a application to spark, it creates a Master process and Master process going to create a bunch of slaves to distribute the work and compute\n",
    "    * Master is Driver\n",
    "    * Slaves are Executors\n",
    "    * Spark go and ask cluster manager(yarn) to allocate some conatiner to start driver process and aks some more containers to execute the application\n",
    "    * This repeats for each spark submit\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution Modes\n",
    "* If you submit your application from local mode, the driver will be launched on client machine but if you submit as cluster then cluster manage assign a node to create driver process\n",
    "* If you run in client mode, suppose if your machine failed,corrupted and lost, spark submit job fails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> If spark dynamic execution enabled, all the unncessary threads/executors which are not in use will be killed/dead</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Transformations\n",
    "* Spark Dataframes/rdds are immutable then how do you process/compute?\n",
    "    * Well, as Dataframes/RDD's immutable, we use transaformations to transform and create new dataframes/rdds.\n",
    "    * Transformations are lazy, then how does it evaluate?. Yes , transformations will get evaluated only when an action is invoked\n",
    "    * There are tow types of transformations\n",
    "        * Narrow transformatios\n",
    "            * A Transformation performed on a single partition and produces a valid results, such as where,etc..\n",
    "        * Wider transformations\n",
    "            * A Transformation that requires data from other partition and produces a valid results, such as Group By,Distinct ,etc.\n",
    "            * It shuffles all partitions data and sort then produce valid results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jobs and Stages\n",
    "* Each action creates a job\n",
    "* Each shuffle creates a stage\n",
    "* The link between each stage is exchange. Exchange is an internal buffer that holds results after any action completes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark RDD\n",
    "* Spark community suggests to use Dataframe instead of RDD because Dataframe has catalyst optimizer over rdd as it optimizes RDD\n",
    "* RDD stands for Resilient,distributed dataset. In simple rdd is a collection, they don't have rows/column structure,schemas.\n",
    "    * <b>Resilient</b> - Fault tolerant, how?. They also store information about how they are created. lets consider an rdd partitioned to one executor for processing. In some times, the executor may fails or crashes.How ever the driver notices the failure and assign the rdd partition to another exector core. The new executor will reload the partition and starts processing it. This is an easy thing, because rdd comes with information how to create it and how to process it.\n",
    "    * In - Simple, RDD can be recreated and reprocessed,that's why we call it as resilient.\n",
    "* If you want to create an RDD, you can create them using spark context\n",
    "* Spark context comes with bunch of methos, binary file,sequence file,and hadoop file and object file\n",
    "* RDD's provide basic transformations map,reduce,flatmap,etc..\n",
    "* The Group by implementation on RDD is not so obvious and might not make sense at first. We needed to handcode everything,including regular opeations such grouping ,aggregations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catalyst Optimizer/SQL Engine\n",
    "* The spark sql engine is a powerfull compiler that optimizes your code and also generates efficient java byte code.\n",
    "* The overall effort of spark sql engine can be broken down into 4 phases.\n",
    "    * <b>Analysis</b>\n",
    "        * The spark sql engine will read your code and generates an abstract syntax tree for your sql/dataframe queries. In this phase your code is analyzed,column names , table and view, sql functions are resolved.\n",
    "    * <b>Logical optiomization</b>\n",
    "        * The sql enginel will apply rule based optimization and construct a set of multiple execution plans\n",
    "        * Then the catalyst optimizer will use cost based optimization to assign a cost to each plan\n",
    "        * Predicate pushdown, boolean experesssion simplifier\n",
    "    * <b>Physical plan</b>\n",
    "        * SQL engine picks most effective logical plan and generates a physical plan. How the plan is going to execute on a cluster\n",
    "    * <b>Code Generation</b>\n",
    "        * This phase involves generating efficent java byte code to run on a each machine/node/executor. This is introduce in spark 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark DataSet API\n",
    "* Dataset API's are language native apis in scala and java.These API's are strongly typed in your jvm based language like scala and they are not all available in dynamically typed languages such as python. If you want to use dataset api then you must your jvm based language scala,java."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Dataframe\n",
    "* csv,json won't come with metadata(column names,data types). We need to explcitly need to define schema\n",
    "* parquet is binary file formationa and it comes with metadata(column names,data types). If you have a choice better to go parquet with snappy compression\n",
    "* Like java, scala, python works on their own datatypes, Spark also work on spark own datatypes and it's required for spark sql engine\n",
    "\n",
    "<b> DataFrame Writer </b>\n",
    "* controls Number of files and file size(maxrecordperfile)\n",
    "* partitions and buckets\n",
    "* sorted data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
